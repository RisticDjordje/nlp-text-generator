{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Sentiment Analysis - Elman RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Firstly, set up the path to the (preprocessed) dataset and glove pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the preprocessed data\n",
    "import os\n",
    "\n",
    "fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "absFilePathToPreprocessedDataset = os.path.join(fileDir, '../db_processed/db_sequences_words_split_short.csv')\n",
    "absFilePathToGloVe = os.path.join(fileDir, '../GloVe/glove.6B.50d.txt')\n",
    "pathToPreprocessedDataset = os.path.abspath(os.path.realpath(absFilePathToPreprocessedDataset))\n",
    "pathToGloveEmbeddings = os.path.abspath(os.path.realpath(absFilePathToGloVe))\n",
    "print (pathToPreprocessedDataset)\n",
    "print (pathToGloveEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the device to run the training on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the learning rate parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Common.TwitterDataset import TwitterDataset\n",
    "\n",
    "# Step #1: Instantiate the dataset\n",
    "# instantiate the dataset\n",
    "dataset = TwitterDataset.load_dataset_and_make_vectorizer(pathToPreprocessedDataset, representation=\"indices\")\n",
    "# get the vectorizer\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Use pre-trained embeddings\n",
    "To use pre-trained embeddings, there are three steps:\n",
    "\n",
    "1. Load the pretrained embeddings\n",
    "2. Select only subset of embeddings for the words that are actually present on the data\n",
    "3. Set the Embedding Layer's weight matrix as the loaded subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step #1.B.1: Load the pre-trained embeddings\n",
    "\n",
    "from Common.PreTrainedEmbeddings import PreTrainedEmbeddings\n",
    "\n",
    "embeddings = PreTrainedEmbeddings.from_embeddings_file(pathToGloveEmbeddings)\n",
    "\n",
    "# Step #1.B.2: Initialize the embedding matrix\n",
    "\n",
    "# get list of words in the vocabulary\n",
    "word_list = vectorizer.text_vocabulary.to_serializable()[\"token_to_idx\"].keys()\n",
    "\n",
    "# get the pre-trained embedding vectors only for words that appear in the vocabulary\n",
    "embeddings_matrix = embeddings.make_embeddings_matrix(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Models.ModelElmanRNN import SentimentClassifierElmanRNN\n",
    "\n",
    "# Step #2: Instantiate the model\n",
    "# instantiate the model\n",
    "model = SentimentClassifierElmanRNN(\n",
    "    embedding_size=50,\n",
    "    num_embeddings=len(vectorizer.text_vocabulary),\n",
    "    rnn_hidden_dim=10,\n",
    "    output_dim=len(vectorizer.target_vocabulary),\n",
    "    padding_idx=vectorizer.text_vocabulary.mask_index,\n",
    "    batch_first=True,\n",
    "    pretrained_embedding_matrix=embeddings_matrix,  # Step #1.B.3: set the loaded subset as a weight matrix\n",
    ")\n",
    "# send model to appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Step #3: Instantiate the loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step #4: Instantiate the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vectorizer.text_vocabulary))\n",
    "print(vectorizer.text_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Common.Trainer import Trainer\n",
    "\n",
    "sentiment_analysis_trainer = Trainer(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the chosen number of epochs\n",
    "num_epochs = 100\n",
    "# setup the chosen batch size\n",
    "batch_size = 20\n",
    "\n",
    "report = sentiment_analysis_trainer.train(num_epochs=num_epochs, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RunHelper import evaluate_model\n",
    "\n",
    "# set the model in eval state\n",
    "model.eval()\n",
    "\n",
    "evaluate_model(sentiment_analysis_trainer, device, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and classifying new data points\n",
    "\n",
    "Let's do inference on the new data. This is another evaluation method to make qualitative judgement about whether the model is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the model on some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RunHelper import run_examples\n",
    "\n",
    "examples = [\n",
    "    \"mr and mrs dursley\"\n",
    "]\n",
    "\n",
    "run_examples(examples, model, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More detailed evaluation on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RunHelper import model_run_and_evaluate\n",
    "\n",
    "model_run_and_evaluate(dataset, vectorizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLPEnvironment')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "orig_nbformat": 2,
  "pygments_lexer": "ipython3",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "78500fd2187caaa72981f7e885295d72e0ee06b93735da747eedd5a2ff07ed48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
